[
  {
    "title": "Modeling and Control of General Hydraulic Excavator for Human-in-the-loop Automation",
    "link": "https://ieeexplore.ieee.org/document/10356425",
    "authors": "<b>Guangda Chen</b>, Yinghao Gan, J. Chen, S. Shi, W. Chen, <a href='https://scholar.google.com/citations?user=SSBrkpMAAAAJ&hl=en'>Y. Chen</a>, <a href='https://scholar.google.com/citations?hl=en&user=1hI9bqUAAAAJ'>Rong Xiong</a> and Changjie Fan.",
    "venue": "ICTAI 2023",
    "venueLink": "https://ictai.computer.org/2023/",
    "rating": "(CCF-C)",
    "rating_link": "https://ccf.atom.im/",
    "bibtex": "@INPROCEEDINGS{10356425,\n  author={Chen, Guangda and Gan, Yinghao and Chen, Jiayi and Shi, Shuanwu and Chen, Wei and Chen, Yingfeng and Xiong, Rong and Fan, Changjie},\n  booktitle={2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)},\n  title={Modeling and Control of General Hydraulic Excavator for Human-in-the-loop Automation}, \n  year={2023},\n  pages={708-716},\n  doi={10.1109/ICTAI59109.2023.00110}}",
    "abstract": "As labor shortages and safety regulations become more prominent, the need for human-in-the-loop automation of excavators is increasing. To meet this demand, we have developed a comprehensive modeling method for the excavator arm using nonlinear optimization approaches, including a simplified model that maps the task space to the joint space, as well as an equivalent model that maps the joint space to the actuator space. These models were then used to build a feedforward-PID joint velocity controller and a joint trajectory controller combined with position feedback, which forms the core of our proposed semi-automatic control system for the excavator arm. Our deployment scheme is simple and efficient, and has been deployed on two excavators of different makes and sizes. Experiments show that our deployment scheme performs well on both excavators, with an average error of 0.05 rad/s for the velocity controller and less than 5 cm for the trajectory controller. Using our semi-automatic system, we have completed demonstration experiments for precise digging and grading operations. A demonstration video can be found at https://youtu.be/N6I0WZGSF68.",
    "keywords": "",
    "pdf": "pdf/Modeling and Control of General Hydraulic Excavator for Human-in-the-loop Automation.pdf",
    "demo": "https://youtu.be/N6I0WZGSF68"
  },
  {
    "title": "Robot Navigation in Complex and Dynamic Pedestrian Scenarios",
    "sub_title": "复杂动态行人场景下的机器人导航",
    "link": "https://doi.org/10.27517/d.cnki.gzkju.2021.000357",
    "authors": "<b>Guangda Chen</b>.",
    "venue": "University of Science and Technology of China",
    "rating": ". Hefei, China. June, 2021.",
    "award": "PhD Thesis",
    "bibtex": "@phdthesis{cgd-phd,\n  title = {复杂动态行人场景下的机器人导航},\n  author = {陈广大},\n  year = {2021},\n  school = {中国科学技术大学},\n  ADDRESS    = \"合肥\"\n};",
    "pdf": "pdf/phd_cgd.pdf",
    "slides": "pdf/phd_ppt.pdf"
  },
  {
    "title": "Accurate Intrinsic and Extrinsic Calibration of RGB-D Cameras with GP-based Depth Correction",
    "link": "https://ieeexplore.ieee.org/document/8588983",
    "authors": "<b>Guangda Chen</b>, Guowei Cui, Zhongxiao Jin, <a href='https://wufeng02.github.io/'>Feng Wu</a> and Xiaoping Chen.",
    "venue": "IEEE Sensors Journal",
    "venueLink": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7361",
    "rating": "(CAA-B), (Volume: 19 , Issue: 7 , April, 1, 2019. IF: 4.5)",
    "rating_link": "https://www.caa.org.cn/Uploads/image/file/20230414/20230414170847_65463.pdf#page=15",
    "bibtex": "@Article{chen2019accurate,\n  title = {Accurate Intrinsic and Extrinsic Calibration of {RGB}-D Cameras With {GP}-Based Depth Correction},\n  author = {Guangda Chen and Guowei Cui and Zhongxiao Jin and Feng Wu and Xiaoping Chen},\n  journal = {IEEE Sensors Journal},\n  volume = {19},\n  number = {7},\n  pages = {2685--2694},\n  year = 2019,\n  month = {apr},\n  publisher = {IEEE},\n  doi = {10.1109/jsen.2018.2889805},\n  url = {https://doi.org/10.1109%2Fjsen.2018.2889805}\n}",
    "abstract": "In recent years, more and more robots have been equipped with low-cost RGB-D sensors, such as Microsoft Kinect and Intel Realsense, for safe navigation and active interaction with objects and people. In order to obtain more accurate and reliable fused color and depth information (coloured point clouds), not only the intrinsic and extrinsic parameters of color and depth sensor should be precisely calibrated, but also the external corrections of depth measurements are required. In this paper, using motion capture system, we propose a reliable calibration framework that enables the precise estimation of the intrinsic and extrinsic parameters of RGB-D sensors and provide a model-free depth calibration method based on heteroscedastic Gaussian Processes. Compared with the existing depth correction techniques, our method can simultaneously estimate the mean and variance of the depth error at different measurement distances, i.e., the probability distribution of the depth error relative to the measured distance, which is essential in the state estimation problems. To verify the effectiveness of our approach, we conduct a thorough qualitative and quantitative analysis of the major steps of our calibration method, and compare our experimental results with other related work. Furthermore, we demonstrate an experiment about the overall improvement of visual SLAM with a Kinect device calibrated by our calibration technique.",
    "keywords": "RGB-D cameras, calibration, motion capture system",
    "pdf": "pdf/rgbd_cal_j.pdf",
    "pdf2": "pdf/jsen.pdf"
  },
  {
    "title": "Robot Navigation with Map-Based Deep Reinforcement Learning",
    "link": "https://ieeexplore.ieee.org/document/9238090",
    "authors": "<b>Guangda Chen</b>, Lifan Pan, Y. C., P. X., Z. W., P. W., <a href='http://staff.ustc.edu.cn/~jianmin/'>Jianmin Ji</a> and Xiaoping Chen.",
    "venue": "ICNSC 2020",
    "venueLink": "http://www.icnsc2020.org/",
    "rating": "(CAA-B)",
    "rating_link": "https://www.caa.org.cn/Uploads/image/file/20230213/20230213163755_94080.pdf#page=21",
    "bibtex": "@InProceedings{chen2020robot,\n title = {Robot Navigation with Map-Based Deep Reinforcement Learning},\n author = {Chen, Guangda and Pan, Lifan and Chen, Yu'an and Xu, Pei and Wang, Zhiqiang and Wu, Peichen and Ji, Jianmin and Chen, Xiaoping},\n year = {2020},\n pages = {1-6},\n address = {Nanjing, China},\n doi = {10.1109/ICNSC48988.2020.9238090},\n organization = {IEEE}}",
    "abstract": "This paper proposes an end-to-end deep reinforcement learning approach for mobile robot navigation with dynamic obstacles avoidance. Using experience collected in a simulation environment, a convolutional neural network (CNN) is trained to predict proper steering actions of a robot from its egocentric local occupancy maps, which accommodate various sensors and fusion algorithms. The trained neural network is then transferred and executed on a real-world mobile robot to guide its local path planning. The new approach is evaluated both qualitatively and quantitatively in simulation and realworld robot experiments. The results show that the map-based end-to-end navigation model is easy to be deployed to a robotic platform, robust to sensor noise and outperforms other existing  DRL-based models in many indicators.",
    "keywords": "robot navigation, obstacle avoidance, reinforcement learning, occupancy map",
    "pdf": "pdf/ICNSC_2020_paper_11.pdf",
    "demo": "https://youtu.be/Eq4AjsFH_cU",
    "slides": "pdf/ICNSC2020slide.pdf",
    "award": "Best Student Paper Award",
    "award_link": "pdf/ICNSC2020Award.pdf"
  },
  {
    "title": "Deep Reinforcement Learning of Map-Based Obstacle Avoidance for Mobile Robot Navigation",
    "link": "https://link.springer.com/article/10.1007/s42979-021-00817-z",
    "authors": "<b>Guangda Chen</b>, Lifan Pan, Y. C., P. X., Z. W., P. W., Jianmin Ji and Xiaoping Chen.",
    "venue": "SN Computer Science",
    "venueLink": "https://www.springer.com/journal/42979",
    "rating": "(Volume: 2 , Issue: 6 , August, 18, 2021)",
    "rating_link": "",
    "bibtex": "@article{chen2021deep,\n title = {Deep Reinforcement Learning of Map-Based Obstacle Avoidance for Mobile Robot Navigation},\n author = {Chen, Guangda and Pan, Lifan and Chen, Yu'an and Xu, Pei and Wang, Zhiqiang and Wu, Peichen and Ji, Jianmin and Chen, Xiaoping},\n journal = {SN Computer Science},\n volume = {2},\n number = {6},\n pages={1--14},\n year = {2021},\n month = {August},\n publisher = {Springer},\n doi = {10.1007/s42979-021-00817-z},\n url = {https://doi.org/10.1007/s42979-021-00817-z}\n } ",
    "abstract": "Autonomous and safe navigation in complex environments without collisions is particularly important for mobile robots. In this paper, we propose an end-to-end deep reinforcement learning method for mobile robot navigation with map-based obstacle avoidance. Using the experience collected in the simulation environment, a convolutional neural network is trained to predict the proper steering operation of the robot based on its egocentric local grid maps, which can accommodate various sensors and fusion algorithms. We use dueling double DQN with prioritized experienced replay technology to update parameters of the network and integrate curriculum learning techniques to enhance its performance. The trained deep neural network is then transferred and executed on a real-world mobile robot to guide it to avoid local obstacles for long-range navigation. The qualitative and quantitative evaluations of the new approach were performed in simulations and real robot experiments. The results show that the end-to-end map-based obstacle avoidance model is easy to deploy, without any fine-tuning, robust to sensor noise, compatible with different sensors, and better than other related DRL-based models in many evaluation indicators.",
    "keywords": "robot navigation, obstacle avoidance, deep reinforcement learning, grid map",
    "pdf": "pdf/Chen2021_Article_DeepReinforcementLearningOfMap.pdf"
  },


  {
    "title": "Distributed Non-Communicating Multi-Robot Collision Avoidance via Map-Based Deep Reinforcement Learning",
    "link": "https://www.mdpi.com/1424-8220/20/17/4836",
    "authors": "<b>Guangda Chen</b>, Shunyi Yao, Jun Ma, L. P., Y. C., P. X., Jianmin Ji and Xiaoping Chen.",
    "venue": "Sensors",
    "rating": "(Volume: 20, Issue: 17 , August, 27, 2020. IF: 3.5)",
    "venueLink": "https://www.mdpi.com/journal/sensors",
    "bibtex": "@Article{chen2020distributed,\n title = {Distributed Non-Communicating Multi-Robot Collision Avoidance via Map-Based Deep Reinforcement Learning},\n author = {Chen, Guangda and Yao, Shunyi and Ma, Jun and Pan, Lifan and Chen, Yu'an and Xu, Pei and Ji, Jianmin and Chen, Xiaoping},\n journal = {Sensors},\n volume = {20},\n number = {17},\n pages = {4836},\n year = {2020},\n publisher = {Multidisciplinary Digital Publishing Institute},\n doi = {10.3390/s20174836},\n url = {https://www.mdpi.com/1424-8220/20/17/4836}\n }",
    "abstract": "It is challenging to avoid obstacles safely and efficiently for multiple robots of different shapes in distributed and communication-free scenarios, where robots do not communicate with each other and only sense other robots' positions and obstacles around them. Most existing multi-robot collision avoidance systems either require communication between robots or require expensive movement data of other robots, like velocities, accelerations and paths. In this paper, we propose a map-based deep reinforcement learning approach for multi-robot collision avoidance in a distributed and communication-free environment. We use the egocentric local grid map of a robot to represent the environmental information around it including its shape and observable appearances of other robots and obstacles, which can be easily generated by using multiple sensors or sensor fusion. Then we apply the distributed proximal policy optimization (DPPO) algorithm to train a convolutional neural network that directly maps three frames of egocentric local grid maps and the robot's relative local goal positions into low-level robot control commands. Compared to other methods, the map-based approach is more robust to noisy sensor data, does not require robots' movement data and considers sizes and shapes of related robots, which make it to be more efficient and easier to be deployed to real robots. We first train the neural network in a specified simulator of multiple mobile robots using DPPO, where a multi-stage curriculum learning strategy for multiple scenarios is used to improve the performance. Then we deploy the trained model to real robots to perform collision avoidance in their navigation without tedious parameter tuning. We evaluate the approach with multiple scenarios both in the simulator and on four differential-drive mobile robots in the real world. Both qualitative and quantitative experiments show that our approach is efficient and outperforms existing DRL-based approaches in many indicators. We also conduct ablation studies showing the positive effects of using egocentric grid maps and multi-stage curriculum learning.",
    "keywords": "multi-robot navigation, distributed collision avoidance, deep reinforcement learning",
    "pdf": "pdf/DRL_NAV_sensors.pdf",
    "demo": "https://youtu.be/KOb1q23L7-U",
    "demo2": "https://www.bilibili.com/video/BV12f4y1Q7cx",
    "demo3": "https://www.bilibili.com/video/BV12v411C7sM"
  },
  {
    "title": "Crowd-Aware Robot Navigation for Pedestrians with Multiple Collision Avoidance Strategies via Map-based Deep Reinforcement Learning",
    "link": "http://dx.doi.org/10.1109/IROS51168.2021.9636579",
    "authors": "Shunyi Yao∗, <b>Guangda Chen</b>∗, Quecheng Qiu, Jun Ma, Xiaoping Chen and Jianmin Ji.",
    "venue": "IROS 2021",
    "venueLink": "https://www.iros2021.org/",
    "rating": "(TH-B)",
    "rating_link": "https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf#page=12",
    "bibtex": "@inproceedings{yao2021crowd,\n title={Crowd-aware robot navigation for pedestrians with multiple collision avoidance strategies via map-based deep reinforcement learning},\n author={Yao, Shunyi and Chen, Guangda and Qiu, Quecheng and Ma, Jun and Chen, Xiaoping and Ji, Jianmin},\n booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n pages={8144--8150},\n year={2021},\n organization={IEEE}\n }",
    "abstract": "It is challenging for a mobile robot to navigate through human crowds. Existing approaches usually assume that pedestrians follow a predefined collision avoidance strategy, like social force model (SFM) or optimal reciprocal collision avoidance (ORCA). However, their performances commonly need to be further improved for practical applications, where pedestrians follow multiple different collision avoidance strategies. In this paper, we propose a map-based deep reinforcement learning approach for crowd-aware robot navigation with various pedestrians. We use the sensor map to represent the environmental information around the robot, including its shape and observable appearances of obstacles. We also introduce the pedestrian map that specifies the movements of pedestrians around the robot. By applying both maps as inputs of the neural network, we show that a navigation policy can be trained to better interact with pedestrians following different collision avoidance strategies. We evaluate our approach under multiple scenarios both in the simulator and on an actual robot. The results show that our approach allows the robot to successfully interact with various pedestrians and outperforms compared methods in terms of the success rate.",
    "pdf": "pdf/IROS_2021_PedNav.pdf",
    "demo": "https://www.bilibili.com/video/BV1Vb4y1D7R6"
  },
  {
    "title": "Multi-Robot Collision Avoidance with Map-Based Deep Reinforcement Learning",
    "link": "https://ieeexplore.ieee.org/abstract/document/9288300/",
    "authors": "Shunyi Yao∗, <b>Guangda Chen</b>∗, Lifan Pan, Jun Ma, Jianmin Ji and Xiaoping Chen.",
    "venue": "ICTAI 2020",
    "venueLink": "https://ictai2020.org/index.html",
    "rating": "(CCF-C)",
    "rating_link": "https://ccf.atom.im/",
    "bibtex": "@InProceedings{yao2020multi,\n author = {Yao, Shunyi and Chen, Guangda and Pan, Lifan and Ma, Jun and Ji, Jianmin and Chen, Xiaoping},\n booktitle = {Proceedings of the 32th International Conference on Tools with Artificial Intelligence (ICTAI)},\n title = {Multi-Robot Collision Avoidance with Map-based Deep Reinforcement Learning},\n pages = {532--539},\n year = {2020},\n organization = {IEEE}\n }",
    "abstract": "Multi-robot collision avoidance in a communicationfree environment is one of the key issues for mobile robotics and autonomous driving. In this paper, we propose a mapbased deep reinforcement learning (DRL) approach for collision avoidance of multiple robots, where robots do not communicate with each other and only sense other robots' positions and the obstacles around them. We use the egocentric grid map of a robot to represent the environmental information around it, which can be easily generated by using multiple sensors or sensor fusion. The learned policy generated from the DRL model directly maps 3 frames of egocentric grid maps and the robot's relative local goal positions into low-level robot control commands. Compared to other methods, the map-based approach is more robust to noisy sensor data and does not require the expensive movement data of other robots, like velocities, accelerations and paths. We first train a convolutional neural network for the navigation policy in a simulator of multiple mobile robots using proximal policy optimization (PPO), where curriculum learning strategy is used to accelerate the training process. Then we deploy the trained model to real robots to perform collision avoidance in their navigation. We evaluate the approach with various scenarios both in the simulator and on three differential-drive mobile robots in the real world. Both qualitative and quantitative experiments show that our approach is efficient with high success rate. The demonstration video can be found at https://youtu.be/jcLKlEXuFuk.",
    "keywords": "multi-robots collision avoidance, reinforcement learning, egocentric grid map",
    "pdf": "pdf/ICTAI_2020.pdf",
    "demo": "https://youtu.be/jcLKlEXuFuk"
  }
]